{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6355a2824fefe67747317d313e1ceff7d9bb5bb3"
   },
   "source": [
    "# Customer churn\n",
    "The customer churn, also known as customer attrition, refers to the phenomenon whereby a customer leaves a company. Some studies confirmed that acquiring new customers can cost five times more than satisfying and retaining existing customers. As a matter of fact, there are a lot of benefits that encourage the tracking of the customer churn rate, for example:\n",
    "* Marketing costs to acquire new customers are high. Therefore, it is important to retain customers so that the initial investment is not\n",
    "wasted;\n",
    "* It has a direct impact on the ability to expand the company;\n",
    "* etc.\n",
    "\n",
    "In this project our goal is to predict the probability of a customer is likely to churn using machine learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "363257e696333db0e8043db32a842586efa4c517"
   },
   "source": [
    "- <a href='#1'>1. Load Data</a>\n",
    "- <a href='#2'>2. Data Manipulation</a>\n",
    "- <a href='#3'>3. Exploratory Data Analysis</a>\n",
    "    - <a href='#3.1'>3.1. Customer churn in data</a>\n",
    "    - <a href='#3.2'>3.2. Distribution of categorical variables</a>\n",
    "    - <a href='#3.3'>3.3. Distribution of continuous variables</a>\n",
    "    - <a href='#3.4'>3.4. Finding missing values</a>\n",
    "    - <a href='#3.5'>3.5. Correlation matrix</a>\n",
    "    - <a href='#3.6'>3.6. Detecting and handling outliers</a>\n",
    "- <a href='#4'>4. Data preparation</a>\n",
    "- <a href='#5'>5. Feature engineering for the baseline model</a>\n",
    "    - <a href='#5.1'>5.1. Feature importance</a>\n",
    "- <a href='#6'>6. Selecting the machine learning algorithms</a>\n",
    "    - <a href='#6.1'>6.1. Train and build baseline model</a>\n",
    "        - <a href='#6.2'>6.1.1 Splitting the dataset</a>\n",
    "        - <a href='#6.3'>6.1.2. Model fitting</a>\n",
    "    - <a href='#6.4'>6.2. Testing the baseline model</a>\n",
    "    - <a href='#6.4'>6.3. ROC-AUC performance for the models</a>\n",
    "- <a href='#7'>7. Optimization</a>\n",
    "    - <a href='#6.1'>7.1. Implementing a cross-validation based approach</a>\n",
    "    - <a href='#6.2'>7.2. Implementing hyperparameter tuning</a>\n",
    "    - <a href='#6.3'>7.3. Train models with the help of new hyperparameter</a>\n",
    "    - <a href='#6.4'>7.4. Problem with the optimization approach</a>\n",
    "        - <a href='#6.5'>7.4.1. Feature transformation</a>\n",
    "        - <a href='#6.5'>7.4.2. Voting-based ensemble model</a>\n",
    "        - <a href='#6.5'>7.4.2.1. Transformed data</a>\n",
    "        - <a href='#6.5'>7.4.2.2. Untransformed data</a>\n",
    "- <a href='#7'>8. Conclusion</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns #visualization\n",
    "import matplotlib.pyplot as plt #visualization\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import io\n",
    "import plotly.offline as py #visualization\n",
    "py.init_notebook_mode(connected=True) #visualization\n",
    "import plotly.graph_objs as go #visualization\n",
    "import plotly.tools as tls #visualization\n",
    "import plotly.figure_factory as ff #visualization\n",
    "#print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e31a1f5e255d7cc71186e9dacfff9ede2c3070d"
   },
   "outputs": [],
   "source": [
    "# Read the training dataset\n",
    "training_data = pd.read_csv('./data/Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "804e8912acc26c8d5433c6895d5558a5cef37e0d"
   },
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fa4ce063f87e3bc81bd6ab3f15869d35691f2d6"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 lines of the dataset\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ef2859792bed421557d7f6fac885e2c41eb7f9e"
   },
   "source": [
    "We can see that the columns name are not consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2d34a4b6be254da17c9e090c53da2551b9684ae"
   },
   "outputs": [],
   "source": [
    "# Convert all columns heading in lowercase \n",
    "clean_column_name = []\n",
    "columns = training_data.columns\n",
    "for i in range(len(columns)):\n",
    "    clean_column_name.append(columns[i].lower())\n",
    "training_data.columns = clean_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8391109bdfa63b01cca323d779f20216e7a5fbbe"
   },
   "outputs": [],
   "source": [
    "# Drop the irrelevant columns  as shown above\n",
    "training_data = training_data.drop([\"rownumber\", \"customerid\", \"surname\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7fbba566230ccb72786b2598df8e64e6afc14337"
   },
   "outputs": [],
   "source": [
    "#Separating churn and non churn customers\n",
    "churn     = training_data[training_data[\"exited\"] == 1]\n",
    "not_churn = training_data[training_data[\"exited\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b42b2eb5a36452ec41e88401d1d0d92a7af62069"
   },
   "outputs": [],
   "source": [
    "target_col = [\"exited\"]\n",
    "cat_cols   = training_data.nunique()[training_data.nunique() < 6].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols if x not in target_col]\n",
    "num_cols   = [x for x in training_data.columns if x not in cat_cols + target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b79263fd5501831a6eae05b86129d783afbfa5e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b17938d14ff7ba82d3328528671cf04baa080fd8"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcde10ba18d75b19913978bb99e5a634b9e21fa1"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 lines of the dataset\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72cc06774c63810f0d2150f87298b9cf47092870"
   },
   "outputs": [],
   "source": [
    "# View the dimension of the dataset\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0cfdcbac2b59ae53fad76269e1bc58e80f1646d"
   },
   "outputs": [],
   "source": [
    "# Checking for unique value in the data attributes\n",
    "training_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb4e5e7cf0980652b3bd5333eff0edb099d30aee"
   },
   "source": [
    "As we can see the rownumber attribute is just like a counter of records, the customerid attribute is a unique identifier for a given customer and the surname attribute enter also the profiling a customer. So we are going remove them from our dataset they don't give useful information the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fdfe4ceba4092cfa5c884f1d72787ff2ae60092"
   },
   "outputs": [],
   "source": [
    "# Drop the irrelevant columns  as shown above\n",
    "#training_data = training_data.drop([\"rownumber\", \"customerid\", \"surname\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27e6012e459395e225278edd9e9404c2ae9cc90c"
   },
   "outputs": [],
   "source": [
    "# Describe the all statistical properties of the training dataset\n",
    "training_data[training_data.columns[:10]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cfa41ebae65fe16e0d1c30f071ee6af0067a18a"
   },
   "outputs": [],
   "source": [
    "# Median\n",
    "training_data[training_data.columns[:10]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d437c26c50077a10cfa2bf4a174efc86ced9afc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "training_data[training_data.columns[:10]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d81c85a9abfc4275eaa851bac86ede30b4fe2fa0"
   },
   "source": [
    "### Customer churn in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8e4f82768a812994157eb31bfe75dd0365828e9"
   },
   "outputs": [],
   "source": [
    "# Percentage per category for the target column.\n",
    "percentage_labels = training_data['exited'].value_counts(normalize = True) * 100\n",
    "percentage_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bca620b309dbc0a535f6df8e0dd08cd82bb08b9a"
   },
   "outputs": [],
   "source": [
    "# Graphical representation of the target label percentage.\n",
    "total_len = len(training_data['exited'])\n",
    "sns.set()\n",
    "sns.countplot(training_data.exited).set_title('Data Distribution')\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2.,\n",
    "            height + 2,\n",
    "            '{:.2f}%'.format(100 * (height/total_len)),\n",
    "            fontsize=14, ha='center', va='bottom')\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_xlabel(\"Labels for exited column\")\n",
    "ax.set_ylabel(\"Numbers of records\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2ddc7d0fcc7d4b93530c39471ac64bbf1b58e48"
   },
   "source": [
    "From this chart, one can see that there are many records with the target label $0$ and fewer records with the target label $1$. One can see that the data records with a $0$ label are about $79.63 \\%$, whereas $20.37 \\%$ of the data records are labeled $1$. We will use all of these facts in the upcoming sections. For now, we can consider our outcome variable as imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40429784c12a93ee56418c8e2888e2db998b3ca0"
   },
   "source": [
    "### Distribution of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98b7bb51e9f662eb797059865c93a09366928931"
   },
   "outputs": [],
   "source": [
    "#function  for pie plot for customer attrition types\n",
    "def plot_pie(column) :\n",
    "    \n",
    "    trace1 = go.Pie(values  = churn[column].value_counts().values.tolist(),\n",
    "                    labels  = churn[column].value_counts().keys().tolist(),\n",
    "                    hoverinfo = \"label+percent+name\",\n",
    "                    domain  = dict(x = [0,.48]),\n",
    "                    name    = \"Churn Customers\",\n",
    "                    marker  = dict(line = dict(width = 2,\n",
    "                                               color = \"rgb(243,243,243)\")\n",
    "                                  ),\n",
    "                    hole    = .6\n",
    "                   )\n",
    "    trace2 = go.Pie(values  = not_churn[column].value_counts().values.tolist(),\n",
    "                    labels  = not_churn[column].value_counts().keys().tolist(),\n",
    "                    hoverinfo = \"label+percent+name\",\n",
    "                    marker  = dict(line = dict(width = 2,\n",
    "                                               color = \"rgb(243,243,243)\")\n",
    "                                  ),\n",
    "                    domain  = dict(x = [.52,1]),\n",
    "                    hole    = .6,\n",
    "                    name    = \"Non churn customers\" \n",
    "                   )\n",
    "\n",
    "\n",
    "    layout = go.Layout(dict(title = column + \" distribution in customer attrition \",\n",
    "                            plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "                            paper_bgcolor = \"rgb(243,243,243)\",\n",
    "                            annotations = [dict(text = \"churn customers\",\n",
    "                                                font = dict(size = 13),\n",
    "                                                showarrow = False,\n",
    "                                                x = .15, y = .5),\n",
    "                                           dict(text = \"Non churn customers\",\n",
    "                                                font = dict(size = 13),\n",
    "                                                showarrow = False,\n",
    "                                                x = .88,y = .5\n",
    "                                               )\n",
    "                                          ]\n",
    "                           )\n",
    "                      )\n",
    "    data = [trace1,trace2]\n",
    "    fig  = go.Figure(data = data,layout = layout)\n",
    "    py.iplot(fig)\n",
    "#for all categorical columns plot pie\n",
    "#for i in cat_cols :\n",
    "    #plot_pie(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f729bebac9137e15d0cd2d43ca69bfe34545333"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the pie plot for geography column\n",
    "plot_pie(cat_cols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2c3cd053a54100cc1e91a2e84ac4c16d25ece50"
   },
   "source": [
    "The output above shows us that the among the churned customers those who are are geographycally located in Germay have a high rate of churn with $40\\%$, followed by France with $39.8\\%$ and Spain with $20.3\\%$. For non chun customers France is leading with $52.8\\%$, Spain with $25.9\\%$ and Germany with $21.3\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13ccbd83f4b0a964165ba73a02d9972d2d6eea89"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the pie plot for gender column\n",
    "plot_pie(cat_cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b65d866dc7fa21c0d1666979bb77f5ae7a0dcfea"
   },
   "source": [
    "The output above shows us that for the churn customers female have $55.9\\%$, whereas male with $44.1\\%$. For the case of non churn customers $57.3\\%$ are male and $42.7\\%$ are female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c943906fcf1289273b31d7cff5f7bc40c0bf65a"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the pie plot for numofproducts column\n",
    "plot_pie(cat_cols[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bd9c3578deb38ab3f6445a50ee1527fec5f3e5c"
   },
   "source": [
    "The graph above shows that among the churn customers, the rate of those who use one product is very high with $69.2\\%$, followed by  those who use two products with $17.1\\%$, three products with $10.8\\%$, and four products with $2.95\\%$. For non churn customers, customers with two products are $53.3\\%$, one product are $46.2\\%$, and three products are $0.58\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccb246fdfd6f739e0eea5902cfda7255f969af8d"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the pie plot for gender column\n",
    "plot_pie(cat_cols[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aae342eb8481f8d124e93a1d559eafce0ecd02b4"
   },
   "source": [
    "The output above shows us that for the churn customers those who possess a card are $69.9\\%$, whereas those don't possess are $30.1\\%$. For the case of non churn customers $70.7\\%$ possess a card and $29.3\\%$ don't possess a card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fa265cb5d403c00d2f6657acaa45cca4c2d8b71"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the pie plot for geography column\n",
    "plot_pie(cat_cols[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "949e4352c8d174302f05da0d0918077db53f7f36"
   },
   "source": [
    "The output above shows us that the among the churned customers those who are not active members have a high rate of churn with $63.9\\%$, and active members with $36.1\\%$. For non chun customers active members are leading with $55.5\\%$, and non active members with $44.5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2519853e0db4a64ea91fb636cf8d465b003ea10d"
   },
   "source": [
    "### Distribution of the continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1882e6ec71aa9640c56704dfda7f59453908df7e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3582a254f8c20312b615eddf384b37f6227638a6"
   },
   "outputs": [],
   "source": [
    "#function  for histogram for customer churn types\n",
    "def histogram(column) :\n",
    "    trace1 = go.Histogram(x  = churn[column],\n",
    "                          histnorm= \"percent\",\n",
    "                          name = \"Churn Customers\",\n",
    "                          marker = dict(line = dict(width = .5,\n",
    "                                                    color = \"black\"\n",
    "                                                    )\n",
    "                                        ),\n",
    "                         opacity = .9 \n",
    "                         ) \n",
    "    \n",
    "    trace2 = go.Histogram(x  = not_churn[column],\n",
    "                          histnorm = \"percent\",\n",
    "                          name = \"Non churn customers\",\n",
    "                          marker = dict(line = dict(width = .5,\n",
    "                                              color = \"black\"\n",
    "                                             )\n",
    "                                 ),\n",
    "                          opacity = .9\n",
    "                         )\n",
    "    \n",
    "    data = [trace1,trace2]\n",
    "    layout = go.Layout(dict(title =column + \" distribution in customer attrition \",\n",
    "                            plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "                            paper_bgcolor = \"rgb(243,243,243)\",\n",
    "                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                             title = column,\n",
    "                                             zerolinewidth=1,\n",
    "                                             ticklen=5,\n",
    "                                             gridwidth=2\n",
    "                                            ),\n",
    "                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                             title = \"percent\",\n",
    "                                             zerolinewidth=1,\n",
    "                                             ticklen=5,\n",
    "                                             gridwidth=2\n",
    "                                            ),\n",
    "                           )\n",
    "                      )\n",
    "    fig  = go.Figure(data=data,layout=layout)\n",
    "    \n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0c98242ed54004ab15ae020eec2e9c4ac67e472"
   },
   "source": [
    "**Note:** For more information on the different plot just pass the mousse hover the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "767207e153fbf00d2f6c656a2bedc994d2ff9e38"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the histogram for creditscore column \n",
    "histogram(num_cols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "384fd8e46e5f1b25811ee181d7b8d140389f06c1"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the histogram for creditscore column \n",
    "# Pass the mouse hover the graph for more information. \n",
    "histogram(num_cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "328499aa5006631103ee35363f6fded43437e4cf"
   },
   "source": [
    "The graph above shows us that the customers with age of 46 are the most churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd5661911c953aa410bcd5b7eee0ccde95d495ee"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the histogram for tenure column \n",
    "histogram(num_cols[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c134882741b926e1974a22b8bd91e19b6fa30a17"
   },
   "source": [
    "Pass the mouse hover the graph for more information. It shows us that the customers who have been with the bank just for one moth are the most churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "046331445c0c66d8b24596d4bdc54230417c9ada"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the histogram for balance column \n",
    "histogram(num_cols[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "feba4d0e71606b8013206224fb63dcd19faaa8c8"
   },
   "outputs": [],
   "source": [
    "# Calling the function for plotting the histogram for estimatedsalary  column \n",
    "histogram(num_cols[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad255cdc2bc1f7f1f884d04456eaf64433a79938"
   },
   "source": [
    "## Finding missing values\n",
    "In order to find the missing values in the dataset, we need to check each and every data attribute. First, we will try to identify which attribute has a missing or null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45271ccd8ea393eb51be570a64bc2e141c4c4f63"
   },
   "outputs": [],
   "source": [
    "training_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c30f694ca9d9977eb5de68559ddd0aa1924379ad"
   },
   "source": [
    "As one can see there is no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1db85d838b8889e8f4a4ec3bb7a89fab12ef19"
   },
   "source": [
    "## Correlation\n",
    "The term correlation refers to a mutual relationship or association between quantities. So, here, we will find out what kind of association is present among the different data attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4175868c5c601f3f96da273ac37591bba9de2c2d"
   },
   "outputs": [],
   "source": [
    "# Get the correlation matrix of the training dataset\n",
    "training_data[training_data.columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62202a44bc37a9878c5ac03a3fc10c5caac43774"
   },
   "outputs": [],
   "source": [
    "# Visualization of the correlation matrix using heatmap plot\n",
    "sns.set()\n",
    "sns.set(font_scale = 1.25)\n",
    "sns.heatmap(training_data[training_data.columns[:10]].corr(), annot = True,fmt = \".1f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd3e2f4ba7d3168e5e2a62cb77227c153a679bb8"
   },
   "source": [
    "The following facts can be derived from the graph:\n",
    "*  Cells with 1.0 values are highly correlated with each other;\n",
    "* Each attribute has a very high correlation with itself, so all the diagonal values are 1.0;\n",
    "* balance attribute is negatively correlated with numberofproducts attribute. It means one attribute increases as the other decreases, and vice versa.\n",
    "\n",
    "Before moving ahead, we need to check whether these attributes contain any outliers or insignificant values. If they do, we need to handle these outliers, so our next section is about detecting outliers from our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88303ae9ae4eac869247260267450483320ce659"
   },
   "source": [
    "## Detecting and Handling Outliers\n",
    "In this part, we will try to detect outliers and how to handle them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf0c468f01077ca8d1cdbca17238895c6ea89f07"
   },
   "outputs": [],
   "source": [
    "# Function which plot box plot for detecting outliers\n",
    "trace = []\n",
    "def gen_boxplot(df):\n",
    "    for feature in df:\n",
    "        trace.append(\n",
    "            go.Box(\n",
    "                name = feature,\n",
    "                y = df[feature]\n",
    "            )\n",
    "        )\n",
    "\n",
    "new_df = training_data[num_cols[:1]]\n",
    "gen_boxplot(new_df)\n",
    "data = trace\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c2060a8719d15fe510fb8138d917bee701dc293"
   },
   "source": [
    "The box plots above don't  show us any value that are faraway from the min value and also from the max value, so there is  outliers detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "964b359451f463810af4b369c9f2a6b8a85c1a35"
   },
   "outputs": [],
   "source": [
    "# Function which plot box plot for detecting outliers\n",
    "trace = []\n",
    "def gen_boxplot(df):\n",
    "    for feature in df:\n",
    "        trace.append(\n",
    "            go.Box(\n",
    "                name = feature,\n",
    "                y = df[feature]\n",
    "            )\n",
    "        )\n",
    "new_df = training_data[num_cols[1:3]]\n",
    "gen_boxplot(new_df)\n",
    "data = trace\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3b7975b14051bea4b2ff621f9aa026004321a92"
   },
   "source": [
    "The graph tells us that for age there is few outliers. As we can see the two extrem values for the age box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "590df313f0401457b0c97b892cd6ab6848a1576e"
   },
   "outputs": [],
   "source": [
    "# Handling age column outliers\n",
    "ageNew = []\n",
    "for val in training_data.age:\n",
    "    if val <= 85:\n",
    "        ageNew.append(val)\n",
    "    else:\n",
    "        ageNew.append(training_data.age.median())\n",
    "        \n",
    "training_data.age = ageNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c3f0cf5a2c39d23807ed895d3a14989bfabff37"
   },
   "outputs": [],
   "source": [
    "# Function which plot box plot for detecting outliers\n",
    "trace = []\n",
    "def gen_boxplot(df):\n",
    "    for feature in df:\n",
    "        trace.append(\n",
    "            go.Box(\n",
    "                name = feature,\n",
    "                y = df[feature]\n",
    "            )\n",
    "        )\n",
    "new_df = training_data[num_cols[3:]]\n",
    "gen_boxplot(new_df)\n",
    "data = trace\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9de5ac80fda5a9f66a3731ac469d4227efa82bf5"
   },
   "source": [
    "The graph above shows nothing abnormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d40d615d05d7d4422884d04ca9d30c871877d69d"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d7363c69bb1e526ab689cd1b6744358e247dbd3"
   },
   "outputs": [],
   "source": [
    "# One-Hot encoding our categorical attributes\n",
    "list_cat = ['geography', 'gender']\n",
    "training_data = pd.get_dummies(training_data, columns = list_cat, prefix = list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1964181d77d373a6c2c8fac65e645d023f472aee"
   },
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b2231681d043ee6e5988cccc525a063c4c417a5"
   },
   "source": [
    "# Feature engineering for the baseline model\n",
    "In this section, you will learn how to select features that are important in order to develop the predictive model. So right now, just to begin with, we won't focus much on deriving new features at this stage because first, we need to know which input variables / columns / data attributes / features give us at least baseline accuracy. So, in this first iteration, our focus is on the selection of features from the available training dataset.\n",
    "## Finding out Feature importance\n",
    "We need to know which the important features are. In order to find that out, we are going to train the model using the Random Forest classifier. After that, we will have a rough idea about the important features for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f197d9b41a8a4426cd4ca599f787709855bd43e4"
   },
   "outputs": [],
   "source": [
    "# Import the Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c91fc57ee33fdade61ae090ebc47142c47d34731"
   },
   "outputs": [],
   "source": [
    "# We perform training on the Random Forest model and generate the importance of the features\n",
    "X = training_data.drop('exited', axis=1)\n",
    "y = training_data.exited\n",
    "features_label = X.columns\n",
    "forest = RandomForestClassifier (n_estimators = 10000, random_state = 0, n_jobs = -1)\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in range(X.shape[1]):\n",
    "    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "801ef7aab641fe6123e1780cf50cfc74bfa11775"
   },
   "outputs": [],
   "source": [
    "# Visualization of the Feature importances\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X.shape[1]), importances[indices], color = \"green\", align = \"center\")\n",
    "plt.xticks(range(X.shape[1]), features_label, rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4ae8e7693535f1885a3399cc3ba404eeba3d914"
   },
   "source": [
    "The graph above shows the features with the highest importance value to the lowest importance value. It shows the most important features are creditscore, age, tenure, balance,  and so on.\n",
    "We will surely revisit again feature engineering in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e2f533a00846940887b5bbb913d053d081337a8"
   },
   "source": [
    "# Selecting Machine Learning Algorithms\n",
    "Since we are modeling a critic problem for that we need model with high performance possible. Here, we will try a couple of different machine learning algorithms in order to get an idea about which machine learning algorithm performs better. Also, we will perform a accuracy comparison amoung them. \n",
    "As our problem is a classification problem, the algorithms that we are going to choose are as follows:\n",
    "* K-Nearest Neighbor (KNN)\n",
    "* Logistic Regression (LR)\n",
    "* AdaBoost\n",
    "* Gradient Boosting (GB)\n",
    "* RandomForest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67b860b739b39876515ef78893b68f153f9a86fa"
   },
   "source": [
    "## Train and build baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3451eed4e6aa7aa60e09534a50aff3f6398cfcf6"
   },
   "outputs": [],
   "source": [
    "# Import different models \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Scoring function\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9793c25bf705960b926614a89927cada70201ac4"
   },
   "outputs": [],
   "source": [
    "X = training_data.drop('exited', axis=1)\n",
    "y = training_data.exited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6cf4094a888f64498d30545cb007616ddcf0ed2"
   },
   "source": [
    "As you can see in the code above, variable X contains all the columns except the target column entitled *exited*, so we have dropped this column. The reason behind dropping this attribute is that this attribute contains the answer/target/label for each row. Machine algorithms need input in terms of a key-value pair, so a target column is key and all other columns are values. We can say that a certain pattern of values will lead to a particular target value, which we need to predict using an machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a67ba7db5b9af6fa6b5b6fb07801678050892a25"
   },
   "source": [
    "#### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57af1f3d77ad2069bc8233a10bf62f7738c297d5"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bac6262bf3b383be54531df74d2ca174b1a2a47"
   },
   "source": [
    "the code above splits the training  data. We will use $75\\%$ of the training data for actual training purposes, and once training is completed, we will use the remaining $25\\%$ of the training data to check the training accuracy of our trained  model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a94d4f45c1742455a2406dabb6184b1cdfde6bff"
   },
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9204e967fde458489943bd8a5dba6664f44d085"
   },
   "outputs": [],
   "source": [
    "# Initialization of the KNN\n",
    "knMod = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform', algorithm = 'auto', leaf_size = 30, p = 2,\n",
    "                             metric = 'minkowski', metric_params = None)\n",
    "# Fitting the model with training data \n",
    "knMod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21d0fed021b3f34e120b9eecffb51a8b73835d20"
   },
   "outputs": [],
   "source": [
    "# Initialization of the Logistic Regression\n",
    "lrMod = LogisticRegression(penalty = 'l2', dual = False, tol = 0.0001, C = 1.0, fit_intercept = True,\n",
    "                            intercept_scaling = 1, class_weight = None, \n",
    "                            random_state = None, solver = 'liblinear', max_iter = 100,\n",
    "                            multi_class = 'ovr', verbose = 2)\n",
    "# Fitting the model with training data \n",
    "lrMod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c6d810ad7d39ff0212835ae2d58a1d8e102e611"
   },
   "outputs": [],
   "source": [
    "# Initialization of the AdaBoost model\n",
    "adaMod = AdaBoostClassifier(base_estimator = None, n_estimators = 200, learning_rate = 1.0)\n",
    "# Fitting the model with training data \n",
    "adaMod.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec66f9db0c6a6ad58560408c1fb260d13062f1cc"
   },
   "outputs": [],
   "source": [
    "# Initialization of the GradientBoosting model\n",
    "gbMod = GradientBoostingClassifier(loss = 'deviance', n_estimators = 200)\n",
    "# Fitting the model with training data \n",
    "gbMod.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c8766783ffe18005c45ad5aaf90f2a19ec9093a"
   },
   "outputs": [],
   "source": [
    "# Initialization of the Random Forest model\n",
    "rfMod = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "# Fitting the model with training data \n",
    "rfMod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a617a36d5a6a453b4068ea6aaeaaa9096b078c22"
   },
   "source": [
    "## Testing the baseline model\n",
    "Here, we will implement the code, which will give us an idea about how good or how bad our trained models perform in a validation set. We are using the mean accuracy score and the AUC-ROC score.\n",
    "We have generated five different classifiers and, after performing testing for each of them on the validation dataset, which is $25\\%$ of held-out dataset from the training dataset, we will find out which model works well and gives us a reasonable baseline score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddb90a2f600de35706f66d26ea88a6e0bcea08f8"
   },
   "outputs": [],
   "source": [
    "# Compute the model accuracy on the given test data and labels\n",
    "knn_acc = knMod.score(X_test, y_test)\n",
    "# Return probability estimates for the test data\n",
    "test_labels = knMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "# Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "knn_roc_auc = roc_auc_score(y_test, test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "129be5759942e44858ce5ea0eeb9afee50792d14"
   },
   "outputs": [],
   "source": [
    "# Compute the model accuracy on the given test data and labels\n",
    "lr_acc = lrMod.score(X_test, y_test)\n",
    "# Return probability estimates for the test data\n",
    "test_labels = lrMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "# Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "lr_roc_auc = roc_auc_score(y_test, test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c5cbb8ae810310736636b7bfb6ff9706e129c57"
   },
   "outputs": [],
   "source": [
    "# Compute the model accuracy on the given test data and labels\n",
    "ada_acc = adaMod.score(X_test, y_test)\n",
    "# Return probability estimates for the test data\n",
    "test_labels = adaMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "# Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "ada_roc_auc = roc_auc_score(y_test, test_labels , average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5a0b43cc9d94a1cbd0fcbd3bba6ef6b0de73bf9"
   },
   "outputs": [],
   "source": [
    "# Compute the model accuracy on the given test data and labels\n",
    "gb_acc = gbMod.score(X_test, y_test)\n",
    "# Return probability estimates for the test data\n",
    "test_labels = gbMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "# Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "gb_roc_auc = roc_auc_score(y_test, test_labels , average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a67da7160c3941dddd40ed009a507e34cc981f45"
   },
   "outputs": [],
   "source": [
    "# Compute the model accuracy on the given test data and labels\n",
    "rf_acc = rfMod.score(X_test, y_test)\n",
    "# Return probability estimates for the test data\n",
    "test_labels = rfMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "# Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "rf_roc_auc = roc_auc_score(y_test, test_labels , average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b63ab2cbe3f4d15acb5bf744dd238e1e948c9960"
   },
   "outputs": [],
   "source": [
    "models = ['KNN', 'Logistic Regression', 'AdaBoost', 'GradientBoosting', 'Random Forest']\n",
    "accuracy = [knn_acc, lr_acc, ada_acc, gb_acc, rf_acc]\n",
    "roc_auc = [knn_roc_auc, lr_roc_auc, ada_roc_auc, gb_roc_auc, rf_roc_auc]\n",
    "\n",
    "d = {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
    "df_metrics = pd.DataFrame(d, index = models)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dad3b073e37f17f40e7a1bba5ec843c5c1ec9ba1"
   },
   "source": [
    "### ROC-AUC performance for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "266f497921c3a38e6a8a7d188e45823ec92c2e5f"
   },
   "outputs": [],
   "source": [
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, knMod.predict_proba(np.array(X_test.values))[:,1])\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lrMod.predict_proba(np.array(X_test.values))[:,1])\n",
    "fpr_ada, tpr_ada, _ = roc_curve(y_test, adaMod.predict_proba(np.array(X_test.values))[:,1])\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, gbMod.predict_proba(np.array(X_test.values))[:,1])\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rfMod.predict_proba(np.array(X_test.values))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "489fa8ed9e8c13a8b0a3590a8e54b5aacddbe9ea"
   },
   "outputs": [],
   "source": [
    "# Plot the roc curve\n",
    "plt.figure(figsize = (12,6), linewidth= 1)\n",
    "plt.plot(fpr_knn, tpr_knn, label = 'KNN Score: ' + str(round(knn_roc_auc, 5)))\n",
    "plt.plot(fpr_lr, tpr_lr, label = 'LR score: ' + str(round(lr_roc_auc, 5)))\n",
    "plt.plot(fpr_ada, tpr_ada, label = 'AdaBoost Score: ' + str(round(ada_roc_auc, 5)))\n",
    "plt.plot(fpr_gb, tpr_gb, label = 'GB Score: ' + str(round(gb_roc_auc, 5)))\n",
    "plt.plot(fpr_rf, tpr_rf, label = 'RF score: ' + str(round(rf_roc_auc, 5)))\n",
    "plt.plot([0,1], [0,1], 'k--', label = 'Random guessing: 0.5')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve ')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c4ef2b36461b4fd207193c453ab379df6d7b72c"
   },
   "source": [
    "In the code above we used *score()* function of scikit-learn, which give us the mean accuracy score, whereas, the *roc_auc_score()* function  provide us with the ROC-AUC score, which is more significant for us because the mean accuracy score considers only one threshold value, whereas the ROC-AUC score takes into consideration all possible threshold values and gives us the score.\n",
    "\n",
    "As you can see in the code snippets given above, the GradientBoosting with $0.86$ and the AdaBoost with $0.84$ classifiers get a good ROC-AUC score on the validation dataset. Other classifiers, such as logistic regression, KNN, and RandomForest do not perform well on the validation set. From this stage onward, we will work with GradientBoosting and AdaBoost classifiers in order to improve their accuracy score.\n",
    "\n",
    "In the next section, we will see what we need to do in order to increase classification accuracy since we want a model with the high accuracy possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17f6a6c97fbc94b87c2459c464df6be010a791a4"
   },
   "source": [
    "# Optimization\n",
    "In this section, we will use the following techniques in order to improve the accuracy of the classifiers :\n",
    "*  Cross-validation\n",
    "*  Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f9f4c5a9c49335cf90e0224c84b33be8581ab0c"
   },
   "source": [
    "## Implementing a cross-validation based approach\n",
    "Here, we are going to implement K-folds cross-validation. For the value of K, I am going to use $K=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba8f7d3169ba491c23af1be4340ebc7daa5f2a48"
   },
   "outputs": [],
   "source": [
    "# Import the cross-validation module\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Function that will track the mean value and the standard deviation of the accuracy\n",
    "def cvDictGen(functions, scr, X_train = X, y_train = y, cv = 5):\n",
    "    cvDict = {}\n",
    "    for func in functions:\n",
    "        cvScore = cross_val_score(func, X_train, y_train, cv = cv, scoring = scr)\n",
    "        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n",
    "    \n",
    "    return cvDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ddbdb0ae624f24419ad5bdf0ab5b672b532747"
   },
   "outputs": [],
   "source": [
    "mod = [knMod, lrMod, adaMod, gbMod, rfMod]\n",
    "cvD = cvDictGen(mod, scr = 'roc_auc')\n",
    "cvD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62777eedc3d0ea78bb44d3ea62c06a9ac5e47ac2"
   },
   "source": [
    "As we can see, in the above output, GradietBoosting and Adaboot classifier perform well. This cross-validation score helps in order to decide which model we should select and which ones we should not go with.  Based on the mean value and the standard deviation value, we can conclude that our ROC-AUC score does not deviate much, so we are not suffering from the overfitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5c0b0f20c1fea60934a85069fa22483b1e0192a"
   },
   "source": [
    "## Implementing hyperparameter tuning\n",
    "Here, we will look at how we can obtain optimal values for the parameters. So, we are going to use the *RandomizedSearchCV* hyperparameter tuning method. We will implement this method for the AdaBoost and GradientBossting models since they are the one having good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3083433f1396daf7a201240f8171f511a587704"
   },
   "outputs": [],
   "source": [
    "# Import methods\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aedf74caaad71397e38f0c31ae6dd05c50deed44"
   },
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3deee40b3379a7e71f6609062edb50e686dc353"
   },
   "outputs": [],
   "source": [
    "# Possible parameters\n",
    "adaHyperParams = {'n_estimators': [10,50,100,200,420]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d457e35ed0cdf3d83b26e8729bcf4d2a27d747e1"
   },
   "outputs": [],
   "source": [
    "gridSearchAda = RandomizedSearchCV(estimator = adaMod, param_distributions = adaHyperParams, n_iter = 5,\n",
    "                                   scoring = 'roc_auc')\n",
    "gridSearchAda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78afc13baae2284901e2e78c511bc5ff58e33f5a"
   },
   "outputs": [],
   "source": [
    "# Display the best parameters and the score\n",
    "gridSearchAda.best_params_, gridSearchAda.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53802b1fe57561600fc0cbb762d9b7ed96bec5d7"
   },
   "source": [
    "The output above shows that the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b7957bfd634003e059d06b3785ee1ac6b1026e3"
   },
   "source": [
    "### GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66ba9407a6da91fb68b3a64dca59b0a632ca3a88"
   },
   "outputs": [],
   "source": [
    "# Possibles parameters\n",
    "gbHyperParams = {'loss' : ['deviance', 'exponential'],\n",
    "                 'n_estimators': randint(10, 500),\n",
    "                 'max_depth': randint(1,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f6dc6543d405d7f58e9c15c7cf954dbbbb0ef91"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "gridSearchGB = RandomizedSearchCV(estimator = gbMod, param_distributions = gbHyperParams, n_iter = 10,\n",
    "                                   scoring = 'roc_auc')\n",
    "# Fitting the model\n",
    "gridSearchGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "204fa5bfe6fbadfcc158c496850a519e91afb1e4"
   },
   "outputs": [],
   "source": [
    "gridSearchGB.best_params_, gridSearchGB.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ea29e38da1107ee03adf0e39c88ffb014b93432"
   },
   "source": [
    "The output above shows that the optimal  values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ac28dca9909dd8f9f1f29ddcf8d21303e589479"
   },
   "source": [
    "## Train models with help of new hyper parameter\n",
    "Here we are going to use the optimal parameter values that we got from the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4141f7148ea8d376b41a20ca25157dd208e776a1"
   },
   "outputs": [],
   "source": [
    "# GradientBoosting with the optimal parameters\n",
    "bestGbModFitted = gridSearchGB.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab59e098bfc088f309632fd120a5a1c260adae89"
   },
   "outputs": [],
   "source": [
    "# AdaBoost with the optimal parameter\n",
    "bestAdaModFitted = gridSearchAda.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17a0565b509ec7b6c1df158016ac72b69358b0e2"
   },
   "outputs": [],
   "source": [
    "functions = [bestGbModFitted, bestAdaModFitted]\n",
    "cvDictbestpara = cvDictGen(functions, scr = 'roc_auc')\n",
    "cvDictbestpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6082d43595643b817c93514e7283df92f75a2704"
   },
   "outputs": [],
   "source": [
    "# Getting the score GradientBoosting\n",
    "test_labels = bestGbModFitted.predict_proba(np.array(X_test.values))[:,1]\n",
    "roc_auc_score(y_test,test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e20e4517b82c8f884d6d29e3c2022f15d89fa55b"
   },
   "outputs": [],
   "source": [
    "# Getting the score AdaBoost\n",
    "test_labels = bestAdaModFitted.predict_proba(np.array(X_test.values))[:,1]\n",
    "roc_auc_score(y_test,test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c084b774032c2b518bdad128ff0f9543d30ab3c"
   },
   "source": [
    "We can see in the above output that there is no such an improvement.  One can still ask this question : can we further improve the accuracy of the classifiers? Sure, there is always room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a12ef2784c7c1a4733e4dfb6f02674e699e85e29"
   },
   "source": [
    "## Problems with the optimization approach\n",
    "Up until, we did not spend a lot of time on feature engineering. So in our best possible approach, we spent time on the transformation of features engineering. We need to implement a voting mechanism in order to generate the final probability of the prediction on the actual test dataset so that we can get the best accuracy score.\n",
    "\n",
    "These are the two techniques that we need to apply:\n",
    "* Feature transformation\n",
    "* An ensemble ML model with a voting mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc3448848247af75a314b6780bdf17277d31430c"
   },
   "source": [
    "### Feature transformation (Feature engineering)\n",
    "We will apply standard scaler/log transformation to our training dataset. The reason behind this is that we have some attributes that are very skewed and some data attributes that have values that are more spread out in nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "865b5a93a3a2ff7284d27902e01ef5dd27b8ed0f"
   },
   "outputs": [],
   "source": [
    "# Import the log transformation method\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acca714d82db2adaf810b1eda74892e23e868e43"
   },
   "outputs": [],
   "source": [
    "transformer = FunctionTransformer(np.log1p)\n",
    "scaler = StandardScaler()\n",
    "X_train_1 = np.array(X_train)\n",
    "#X_train_transform = transformer.transform(X_train_1)\n",
    "X_train_transform = scaler.fit_transform(X_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0c363d796d013c16b7390e3f8f65111005ebe90"
   },
   "outputs": [],
   "source": [
    "bestGbModFitted_transformed = gridSearchGB.best_estimator_.fit(X_train_transform, y_train)\n",
    "bestAdaModFitted_transformed = gridSearchAda.best_estimator_.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94833466e7080b6edf8c3d982723ca7464147509"
   },
   "outputs": [],
   "source": [
    "cvDictbestpara_transform = cvDictGen(functions = [bestGbModFitted_transformed, bestAdaModFitted_transformed],\n",
    "                                     scr='roc_auc')\n",
    "cvDictbestpara_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1628a9cb057113f3dcb13f16d585ad2b01efb1d3"
   },
   "outputs": [],
   "source": [
    "# For the test set\n",
    "X_test_1 = np.array(X_test)\n",
    "#X_test_transform = transformer.transform(X_test_1)\n",
    "X_test_transform = scaler.fit_transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04465a94b91e59b63179537d9bd5415ed4ace314"
   },
   "outputs": [],
   "source": [
    "test_labels=bestGbModFitted_transformed.predict_proba(np.array(X_test_transform))[:,1]\n",
    "roc_auc_score(y_test,test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40c4237e55709e56b6c1567e88539739f06132dd"
   },
   "source": [
    "### Voting-based ensemble model\n",
    "In this section, we will use a voting-based ensemble classifier. So, we implement a voting-based machine learning model for both untransformed features as well as transformed features. Let's see which version scores better on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dc913a004c6dfc93a37025a37b9f60c6d20f3eb"
   },
   "source": [
    "#### For transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6df96f5dbe5dd254b7697076741601a99753c3f6"
   },
   "outputs": [],
   "source": [
    "# Import the voting-based ensemble model\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7f5da672893f703aab5198a4cbeae8e0ac4afde"
   },
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "votingMod = VotingClassifier(estimators=[('gb', bestGbModFitted_transformed), \n",
    "                                         ('ada', bestAdaModFitted_transformed)],\n",
    "                                         voting = 'soft', weights = [2,1])\n",
    "# Fitting the model\n",
    "votingMod = votingMod.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f792fa91bbe1d59c513ed8437c546cffa29c6b29"
   },
   "outputs": [],
   "source": [
    "test_labels=votingMod.predict_proba(np.array(X_test_transform))[:,1]\n",
    "votingMod.score(X_test_transform, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34a24bc7a69632d4d926b091a687c7825bd192fd"
   },
   "outputs": [],
   "source": [
    "# The roc_auc score\n",
    "roc_auc_score(y_test, test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eeadbe62eddc5d689f155fce406f0acfa5bf2518"
   },
   "source": [
    "#### For untransform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f87e7db61b3418f372f360cd2fd525129726a978"
   },
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "votingMod_old = VotingClassifier(estimators = [('gb', bestGbModFitted), ('ada', bestAdaModFitted)], \n",
    "                                 voting = 'soft', weights = [2,1])\n",
    "# Fitting the model\n",
    "votingMod_old = votingMod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d7e3eb9a63ba09fd6ffd55e209f45f6869d9e56"
   },
   "outputs": [],
   "source": [
    "test_labels = votingMod_old.predict_proba(np.array(X_test.values))[:,1]\n",
    "votingMod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36b90dbf52039d7918d7fdf690d703b9f79cb5f0"
   },
   "outputs": [],
   "source": [
    "# The roc_auc score\n",
    "roc_auc_score(y_test,test_labels , average = 'macro', sample_weight = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7596151a11f3be511b3b44adcfaa97f9eb01c8b8"
   },
   "source": [
    "In the both cases above we have achieved about $87\\%$ accuracy.  This score is by far the most efficient accuracy as per industry standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e6a8ef86aacc4973eb38c8d4c06c5e7be1f3b20"
   },
   "source": [
    "# Conclusion\n",
    "In this project we build a model that predict how likely a customer is going to churn. During exploratory data analysis we found out that the female customer are the most likely to churn, customer that are located in Germany are the most churned, and also customer using only one product are the most churned. After building several model we ended up with two  GradientBoosting and AdaBoost which performed better than others followed by Random Forest. We dicided to go further with the two and implemented a voting-based for the two which will allow us to choose the best model. Since the problem is about binary classification with a imbalance dataset, I have used the most efficient metric for model performance which is the ROC-AUC score and my model achieved about $87\\%$ accuary. This score is by far the most efficient accuracy as per industry standards. The model can achieve better performance providing a lot of historical data for the training phase. For the next way to explore is may be try to group value in tenure column in trimester, semester and yearly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
